\chapter{Estado del Arte}
\label{chapter:Estado del Arte}


%%% SECTION
\section{Introducción}
El objetivo de este capítulo es realizar un análisis de los diferentes avances, desarrollos y tecnologías disponibles en el ámbito de la solución planteada. 

Este análisis tiene como objetivo identificar enfoques y metodologías en distintas áreas, como la extracción de información a partir de fuentes web, el análisis y procesado de texto y el uso de técnicas de Inteligencia Artificial aplicadas al Procesamiento de Lenguaje Natural.

De esta forma se puede establecer un contexto para el problema a resolver y justificar la elección de las tecnologías y metodologías a utilizar en el desarrollo de la solución propuesta.

\section{Problemática a resolver}

La búsqueda de ayudas y subvenciones es una tarea que la mayoría de las empresas, sobre todo las que tienen menos recursos, realizan en su día a día.
Para ello, existen diferentes plataformas de ayudas a empresas, algunas nacionales y otras de carácter internacional. 
Sin embargo, esta tarea puede resultar complicada y tediosa, ya que implica una búsqueda constante de nuevas posibilidades de financiacióna través de distintas fuentes.
Además, la información sobre estas convocatorias suele estar distribuidas en diferentes fuentes, desde las propias plataformas a documentación oficial del estado.
Esto supone que a la hora de realizar una búsqueda de posibles convocatorias de financiación, se acabe con un conjunto de fuentes con diferentes estructuras y formatos.

En la mayoría de los casos, las convocatorias suelen tener asociados diferentes documentos, en su mayoría en formato PDF, los cuales pueden ser extensos, y además usan un lenguaje técnico, típico de este tipo de documentos, que dificulta su comprensión.
Esto al final supone una complicación por parte de las empresas a la hora de acceder a información clave de las convocatorias de forma mas rápida, como requisitos, plazos, presupuesto o condiciones de participación.
Estos problemas de accesibilidad y estandarización de las convocatorias de ayudas suponen una barrera de acceso importante, que reduce las oportunidades de acceso a financiación para algunas empresas, y suponen una inversión en tiempo y esfuerzo en la tarea de búsqueda y filtrado por parte de éstas.

El desarrollo planteado en este proyecto pretende ser una solución a esta problemática, proporcionando una herramienta que sea capaz de identificar y extraer la documentación de las convocatorias, y aplicar técnicas de Inteligencia Artificial para extraer la información clave y dotarla de una estructura mas estandarizada, así como permitir la consulta de esta información de forma sencilla a partir de un agente conversacional.


\section{Soluciones disponibles}

Dejando a parte de momento las soluciones basadas en Inteligencia Artificial, las cuales se comentarán en secciones posteriores, existen diferentes metodologías para la búsqueda de información sobre estas convocatorias:

\subsection{Plataformas de convocatorias}

Existen diferentes plataformas que recopilan información sobre convocatorias de ayudas y subvenciones, a las cuales las empresas pueden acceder para explorar las diferentes opciones de convocatorias, y valorar si se ajustan a su situación.
Estos portales de convocatorias suelen estar disponibles en plataformas tanto gubernamentales como privadas, que recopilan y organizan la información sobre diferentes ayudas disponibles. 
Además, estas herramientas suelen permitir aplicar filtros en las búsquedas por diferentes características, como el área geográfica, el perfil de la empresa solicitante, o el tipo de ayuda.

\begin{itemize}

    \item \textbf{Portales de ayudas gubernamentales}:\\
    Las diferentes instituciones públicas suelen ofrecer portales informativos donde publican este tipo d econvocatorias, ya sean a nivel local, regional o nacional. Estos portales permiten visualizar estas voncovatorias, pero a un nivel básico en cuanto a experiencia de usuario, y aunque la totalidad de la información siempre está disponible, ya sea en el propio portal o mediante enlaces a diferentes fuentes documentales, el análisis y búsqueda de información clave es tediosa y lenta.    
    
    \begin{itemize}
        \item CDTI: Centro para el Desarrollo Tecnológico y la Innovación\cite{cdti}.
        \item Grupo SPRI\cite{spri}.
        \item SODERCAN: Sociedad para el desarrollo ragional de Cantabria\cite{sodercan}.
        \item Portal de ayudas del Ministerio para la Transofrmación Digital y de la Función Pública\cite{ayudasgob}.
        \item Andalucía Trade: Incentivos para Desarrollo Induatrial y Proyectos de I+D+i \\Empresarial\cite{atrade}.
    \end{itemize}

    \item \textbf{Plataformas privadas de información sobre subvenciones}:\\
    Algunas empresas recopilan información sobre ayudas a empresas, las estructuran en bases de datos y ofrecen el acceso a esta información como servicio, garantizando en éste la calidad de la información y una actualización constante del listado de ayudas disponibles.
    El inconveniente de estas plataformas es que, pese a ofrecer servicios de búsqueda que suelen tener interfaces mas amigables e información mas directa, suelen ser herramientas de pago, y el acceso completo a la información puede supoer un coste económico adicional.
    Algunos ejemplos de estas plataformas son Fandit \cite{fandit} u OpenGrants \cite{opengrants}.

\end{itemize}


\subsection{Web Scraping}

Una solución alternativa a la búsqueda manual en portales de ayudas es el uso de herramientas de Web Scraping.
El Web Scraping \cite{inbook} es una técnica utilizada para extraer información de sitios web de manera automatizada. Consiste en el uso de programas o scripts que navegan por páginas web, recopilan datos estructurados y los almacenan en un formato más accesible, como bases de datos o archivos locales JSON o CSV, por ejemplo. Esta práctica es ampliamente utilizada en diversos sectores para la recopilación y análisis de información a gran escala.

Generalmente el proceso de Web Scraping se desarrolla en varias etapas:

\begin{itemize}
    \item Solicitud HTTP: La herramienta de scraping envía una solitcitud HTTP a una página web para obtener su contenido.
    \item Extracción de datos: Se analiza el código fuente de la página, y según la configuración establecida en el scraper, se extraen los datos requeridos mediante comandos de parseo propios de la herramienta, expresiones regulares, o bots de navegación automatizada.
    \item Almacenamiento de la información: Una vez obtenidos los datos, estos se pueden formatear y almacenar según convenga en el caso de uso. 
\end{itemize}

Existen diferentes metodologías de Web Scraping:

\begin{itemize}
    \item \textbf{Análisis HTML}: En múltiples sitios web, se generan automáticamente grandes volúmenes de páginas a partir de fuentes de datos estructuradas, como bases de datos, mediante scripts o plantillas que organizan la información en formatos homogéneos. 
    En minería de datos, un wrapper es un programa que identifica plantillas en una fuente de datos, extrae su contenido y lo transforma en una estructura relacional. 
    La inducción de wrappers asume que las páginas de entrada siguen un patrón identificable, usualmente a través de formatos de URL comunes. 
    Además, lenguajes de consulta para datos semiestructurados, como XQuery y HTQL, permiten analizar, extraer y modificar información en sitios web HTML \cite{miningweb}.

    \item \textbf{Análisis DOM}: Los programas pueden acceder a contenido dinámico generado por scripts del lado del cliente mediante la integración de un navegador web, como Internet Explorer o Mozilla browser control \cite{domparsing}. 
    Estas aplicaciones analizan las páginas web y las estructuran en un árbol del Document Object Model (DOM), lo que permite extraer secciones específicas del contenido.
    El modelo DOM organiza una página web en una estructura arbórea, permitiendo su interpretación y almacenamiento a partir de una dirección web especificada, como ocurre en los motores de búsqueda. 
    Este enfoque ofrece gran flexibilidad y agilidad, ya que permite rastrear elementos presentes en la página sin depender de que el equipo de desarrollo web los exponga explícitamente en la capa de datos.

    \item \textbf{HTML DOM (Hyper Text Markup Language Document Object Model)}: Es un estándar para la obtención, manipulación y modificación de elementos HTML \cite{Gunawan}. 
    Define objetos y propiedades para cada componente HTML, así como métodos para acceder a ellos, optimizando la eficiencia del DOM.
    JavaScript, como lenguaje principal, permite acceder y manipular todos los elementos de un documento HTML a través del DOM. 
    En este modelo, cada elemento HTML se trata como un objeto, cuya interfaz de programación está compuesta por métodos y propiedades específicas.

    \item \textbf{Expresiones regulares (RegEx)}: Las expresiones regulares son fórmulas que definen patrones específicos para identificar conjuntos de caracteres en diversas cadenas de texto \cite{oreilywebacraping}. 
    Se componen de caracteres ordinarios y metacaracteres, los cuales modifican la interpretación del patrón.
    Aunque su sintaxis puede parecer compleja, las expresiones regulares son una herramienta esencial para el análisis y procesamiento de datos en cadenas de texto, por lo que es fundamental comprenderlas al menos a nivel básico.
    
    \item \textbf{XPath}: XPath es el componente principal del estándar XSLT (Stylesheet Language Transformation) y se utiliza para navegar y seleccionar elementos y atributos dentro de documentos XML \cite{asikri}. 
    Además, puede aplicarse en documentos HTML.
    XPath funciona como un lenguaje de selección de nodos en estructuras XML, siendo la expresión más utilizada la ruta de ubicación (location path). 
    Esta ruta emplea al menos un paso de ubicación para identificar un conjunto de nodos dentro de un documento. 
    La forma más simple es la selección del nodo raíz del documento, representada por el símbolo "/" , que también es el indicador del directorio raíz en sistemas de archivos Unix.
    
    \item \textbf{Reconocimiento de anotaciones semánticas}: Las páginas extraídas pueden incluir metadatos, marcas semánticas y anotaciones que permiten identificar datos específicos \cite{inbook}. 
    Por ejemplo, esta técnica puede considerarse un caso particular del análisis DOM si las anotaciones están integradas en las páginas, como ocurre con Microformat. 
    En otro caso, las anotaciones se almacenan y gestionan de manera independiente de las páginas web, organizándose en una capa semántica, de modo que los scrapers pueden obtener el esquema e instrucciones desde esta capa antes de realizar el raspado de las páginas.


\end{itemize}

En el ámbito del Web Scraping, específicamente con el lenguaje de Programación Python, podemos encontrar las siguientes librerías:

\begin{itemize}
    \item \textbf{BeautyfulSoup}: BeautifulSoup es una biblioteca de Python diseñada para el análisis, extracción y manipulación de datos en documentos HTML y XML. 
    Su funcionamiento se basa en la creación de un árbol de análisis sintáctico (parse tree), que estructura el contenido de la página web de manera jerárquica, permitiendo navegar por los nodos, buscar elementos específicos y modificar el contenido.
    BeautifulSoup admite múltiples analizadores (parsers), como lxml, html.parser y html5lib, cada uno con diferentes niveles de velocidad y compatibilidad. 
    Su sintaxis flexible permite localizar elementos a través de etiquetas, atributos y selectores CSS, facilitando la extracción de datos estructurados de páginas web. Además, cuenta con métodos para limpiar el contenido, eliminar etiquetas HTML y exportar la información en diversos formatos \cite{9274270}.

    \item \textbf{Scrapy}: Scrapy es un framework de Python diseñado para la extracción estructurada de datos mediante web scraping y crawling. 
    Su arquitectura modular permite gestionar solicitudes HTTP, procesar respuestas y almacenar datos de manera eficiente. 
    Scrapy opera a través de un flujo de trabajo basado en spiders, que son clases definidas por el usuario encargadas de especificar la lógica de extracción \cite{domparsing}.
    
    El motor de Scrapy (Scrapy Engine) coordina los componentes principales:
    
    \begin{itemize}
        \item Scheduler, que organiza las solicitudes pendientes.
        \item Downloader, que ejecuta las peticiones HTTP y recibe las respuestas.
        \item Spiders, que analizan y extraen información relevante.
        \item Item Pipeline, que transforma, valida y almacena los datos obtenidos en formatos como JSON, CSV o bases de datos SQL y NoSQL.
    \end{itemize}

    Además, Scrapy admite el uso de middlewares, tanto en el Downloader como en el Spider, para modificar solicitudes y respuestas, gestionar sesiones y evitar bloqueos mediante técnicas como rotación de proxies y user agents. 
    Su diseño asincrónico optimiza el rendimiento, permitiendo la extracción masiva de datos con alta eficiencia.

    \item \textbf{Selenium}: Selenium es un framework de automatización de navegadores de código abierto utilizado para la ejecución de pruebas y la extracción de datos mediante web scraping \cite{9142938}. 
    Su funcionamiento se basa en la interacción con páginas web a través de un WebDriver, que actúa como un controlador para manipular elementos de la interfaz de usuario, simular clics, completar formularios y desplazarse por el contenido dinámico generado mediante JavaScript.
    El ecosistema de Selenium está compuesto por varios módulos:

    \begin{itemize}
        \item Selenium WebDriver, que permite la automatización de navegadores como Chrome, Firefox y Edge mediante controladores específicos.
        \item Selenium Grid, que posibilita la ejecución distribuida de pruebas y scraping en múltiples máquinas.
        \item Selenium IDE, una extensión que facilita la grabación y reproducción de secuencias de prueba en navegadores.
    \end{itemize}

    Para realizar web scraping con Selenium, se inicia una sesión de navegador con el WebDriver, se navega a la URL objetivo, y se localizan los elementos deseados mediante selectores XPath o CSS. A diferencia de frameworks como Scrapy o BeautifulSoup, Selenium es ideal para interactuar con sitios que requieren ejecución de JavaScript o carga dinámica de contenido, aunque su rendimiento puede ser inferior debido a la sobrecarga computacional del manejo de un navegador real.

\end{itemize}

\subsection{Procesamiento de Lenguaje Natural}

El Procesamiento de Lenguaje Natural (NLP) es una disciplina de la inteligencia artificial y la lingüística computacional que permite a las máquinas interpretar, comprender, generar y manipular el lenguaje humano de manera estructurada \cite{nlpstate}. 
Su aplicación abarca desde la traducción automática y el análisis de sentimientos hasta la generación de texto y los asistentes virtuales.

El Procesamiento de Lenguaje Natural emplea diversas técnicas computacionales para la interpretación y manipulación del lenguaje humano. 
Estas técnicas pueden dividirse en métodos estadísticos, basados en reglas y de aprendizaje profundo, siendo ampliamente utilizadas en tareas como la clasificación de textos, el análisis de sentimientos y la traducción automática.

\begin{itemize}
    \item \textbf{Preprocesamiento de Texto}
        \begin{itemize}
            \item \textbf{Tokenización}: La tokenización es un proceso esencial en el Procesamiento de Lenguaje Natural que divide un texto en unidades denominadas tokens. 
            Existen varios tipos \cite{schmidt2024tokenizationcompression}, siendo los principales: tokenización por palabras, que separa el texto en palabras individuales y es efectiva en idiomas con delimitadores claros como el inglés; 
            tokenización por subpalabras, utilizada en modelos como BERT y GPT, que emplea técnicas como Byte Pair Encoding (BPE) para manejar vocabularios extensos y palabras fuera de vocabulario (OOV); 
            tokenización por oraciones, que segmenta el texto en unidades sintácticas más grandes basándose en puntuación y reglas lingüísticas; y tokenización por caracteres, útil en modelos de aprendizaje profundo y generación de texto. 
            La selección del método adecuado depende del idioma y la tarea específica, siendo crucial en aplicaciones como traducción automática, análisis de sentimientos y recuperación de información.
            
            \item \textbf{Lematización y Stemming}: Son técnicas empleadas para normalizar palabras reducciéndolas a su raíz morfológica \cite{steminglemmatization}. 
            El stemming elimina afijos mediante reglas predefinidas, sin considerar el contexto, lo que lo hace rápido pero propenso a errores (ejemplo: "running" → "run", pero "better" → "bet"). 
            En contraste, la lematización utiliza análisis morfológico y diccionarios lingüísticos para obtener la forma base correcta (ejemplo: "better" → "good"), ofreciendo mayor precisión. 
            Mientras que el stemming es más eficiente en grandes volúmenes de datos, la lematización es preferida en tareas como análisis de sentimientos, recuperación de información y traducción automática, donde la precisión semántica es crucial.
            
            \item \textbf{Eliminación de stopwords}: Las stopwords son palabras de alta frecuencia en un idioma que generalmente no aportan significado relevante en tareas de Procesamiento de Lenguaje Natural, como artículos, preposiciones y pronombres (ejemplo: "el", "de", "y", "pero"). 
            Se eliminan para reducir la dimensionalidad del texto y mejorar la eficiencia de los modelos de análisis de texto \cite{Sarica_2021}. 
            Bibliotecas como NLTK, SpaCy y Scikit-learn incluyen listas de stopwords predefinidas, aunque pueden personalizarse según la aplicación. 
            Si bien su eliminación es útil en tareas como búsqueda de información y clasificación de textos, en ciertos casos, como en análisis de sentimientos o generación de texto, pueden ser necesarias para preservar el contexto semántico.
        \end{itemize}

        \item \textbf{Análisis Morfosintáctico y Semántico}
        \begin{itemize}
            \item \textbf{Etiquetado gramatical (POS Tagging)}: Es una técnica del Procesamiento de Lenguaje Natural que asigna a cada palabra de un texto su categoría gramatical correspondiente (sustantivo, verbo, adjetivo, etc.) en función de su contexto \cite{postagging}. 
            Se basa en reglas lingüísticas, modelos estadísticos o redes neuronales para mejorar la precisión del análisis. 
            Herramientas como NLTK, SpaCy y Stanford NLP utilizan algoritmos como Hidden Markov Models (HMM) o Redes Neuronales Recurrentes (RNNs) para realizar esta tarea. 
            El POS Tagging es clave en aplicaciones como análisis de sentimientos, desambiguación semántica y traducción automática, ya que ayuda a comprender la estructura y significado del lenguaje.
            
            \item \textbf{Parsing sintáctico}: El parsing sintáctico o análisis sintáctico consiste en la construcción de la estructura jerárquica de una oración para comprender su sintaxis, identificando la relación jerárquica entre las palabras mediante árboles sintácticos o dependencias gramaticales \cite{zhang2020surveysyntacticsemanticparsingbased}. 
            Existen dos enfoques principales: el parsing basado en constituyentes, que descompone la oración en frases (sintagmas nominales, verbales, etc.), y el parsing basado en dependencias, que representa las relaciones entre palabras mediante un grafo dirigido. 
            Herramientas como NLTK, SpaCy y Stanford Parser emplean algoritmos como CYK, Earley o modelos de redes neuronales para esta tarea. 
            El parsing sintáctico es esencial en aplicaciones como traducción automática, generación de texto y comprensión del lenguaje natural, donde la estructura de la oración influye en su interpretación.
            
            \item \textbf{Reconocimiento de Entidades Nombradas (NER)}: Es una técnica que tiene como objetivo identificar y clasificar entidades dentro de un texto, como nombres de personas, lugares, organizaciones, fechas, entre otros \cite{roy2021recenttrendsnamedentity}. 
            Utiliza enfoques basados en reglas lingüísticas, modelos estadísticos o aprendizaje automático para detectar estas entidades en el contexto del texto. 
            Herramientas como SpaCy, NLTK y Stanford NER emplean modelos entrenados en grandes corpus de datos para reconocer entidades y asignarles una etiqueta adecuada. 
            El NER es crucial en aplicaciones como extracción de información, análisis de noticias, y búsqueda semántica, ya que facilita la identificación y categorización de información relevante dentro de grandes volúmenes de datos.
        \end{itemize}

        \item \textbf{Representación de Texto}
        \begin{itemize}
            \item \textbf{Bag of Words (BoW)}: Es una técnica de representación de texto que convierte éste en una matriz de características, donde cada documento se representa como un conjunto de palabras sin tener en cuenta el orden o la gramática \cite{bow}. 
            En el modelo BoW, cada palabra única en el corpus se convierte en una característica (o columna) y cada documento se representa como un vector en el que el valor de cada entrada corresponde a la frecuencia de la palabra en ese documento. 
            Aunque es simple y eficiente, BoW presenta varios inconvenientes, como la pérdida de contexto y el orden de las palabras, la alta dimensionalidad, la falta de captura de relaciones semánticas y la presencia de ruido en los datos, lo que puede afectar la precisión y la interpretación del modelo. 
            Esta técnica, sin embargo, sigue siendo útil en tareas como clasificación de texto, análisis de sentimientos y recuperación de información, pero sus limitaciones han llevado al desarrollo de enfoques más avanzados.
            
            \item \textbf{TF-IDF (Term Frequency - Inverse Document Frequency)}: Método que pondera términos relevantes dentro de un corpus \cite{tfidf}. 
            Es una técnica de representación de texto que asigna un peso a cada término de un documento, basándose en dos componentes: la frecuencia de término (TF), que mide cuántas veces aparece un término en un documento, y la frecuencia inversa de documento (IDF), que mide la importancia de un término dentro de un conjunto de documentos. 
            El cálculo de TF es sencillo y se basa en la cantidad de veces que un término aparece en un documento en comparación con el total de términos del documento, mientras que IDF ajusta el peso del término según su frecuencia en todos los documentos, penalizando las palabras comunes que aparecen en muchos documentos. 
            El resultado es un valor que refleja la relevancia de un término en un documento en particular dentro de un corpus. 
            TF-IDF es ampliamente utilizado en tareas como clasificación de texto, búsqueda de información y análisis de contenido, ya que ayuda a identificar términos significativos que son relevantes en el contexto de un conjunto de documentos. 
            Sin embargo, también presenta algunas limitaciones, como la incapacidad para capturar relaciones semánticas entre palabras y su dependencia de la estructura del corpus.
            
            \item \textbf{Word Embeddings}: Técnicas avanzadas de representación de palabras que convierte las palabras en vectores numéricos de alta dimensión \cite{almeida2023wordembeddingssurvey}. 
            A diferencia de las representaciones tradicionales como Bag of Words o TF-IDF, que tratan las palabras de manera independiente, los word embeddings capturan las relaciones semánticas y contextuales entre palabras, representando palabras similares en espacios vectoriales cercanos. 
            Modelos como Word2Vec, GloVe, FastText o los posteriores modelos de Embeddings asociados a Grandes Modelos del Lenguaje aprenden estas representaciones mediante redes neuronales entrenadas sobre grandes corpus de texto, aprovechando el contexto de las palabras en las oraciones para generar sus vectores. 
            Los word embeddings permiten capturar propiedades lingüísticas, como sinónimos, analogías y jerarquías semánticas, mejorando el rendimiento en tareas como traducción automática, análisis de sentimientos, clasificación de texto y respuestas automáticas. 
            Aunque potentes, los embeddings también presentan desafíos, como la dificultad para representar términos poco frecuentes o palabras con múltiples significados (polisemia).
        
        \end{itemize}

        \item \textbf{Modelos de Aprendizaje Automático y Profundo}
        \begin{itemize}
            \item \textbf{Modelos basados en aprendizaje automático clásico}: Utilización de algoritmos tanto supervisados como no supervisados, como regresiones logísticas, árboles de decisión, SVM, k-means o Random Forest, entre otros, para tareas de clasificación o clusterización de texto \cite{nlpml}.
            Estos modelos estan diseñados para trabajar con datos numéricos, por lo que es necesario aplicar técnicas de representación numérica de texto como las ya anteriormente comentadas.
            
            \item \textbf{Redes Neuronales Recurrentes (RNN) y LSTM}: Modelos diseñados para procesar secuencias de texto y capturar dependencias contextuales.
            Son arquitecturas de Deep Learning diseñadas para procesar datos secuenciales, lo que las hace especialmente útiles en Procesamiento de Lenguaje Natural. 
            A diferencia de los modelos tradicionales de Machine Learning, las RNN pueden capturar dependencias temporales en el texto, ya que su estructura permite que la información de estados previos influya en la interpretación de los siguientes \cite{schmidt2019recurrentneuralnetworksrnns}. 
            Sin embargo, las RNN convencionales presentan problemas con secuencias largas debido a la desaparición o explosión del gradiente. 
            Para solucionar esto, surgieron las LSTM, que incorporan puertas de memoria que regulan el flujo de información, permitiendo recordar dependencias a largo plazo de manera más eficiente \cite{staudemeyer2019understandinglstmtutorial}. 
            Estas redes han sido ampliamente utilizadas en tareas como traducción automática, generación de texto, análisis de sentimientos y reconocimiento de voz. 
            Aunque las LSTM han mejorado el manejo de secuencias largas, han sido en gran parte reemplazadas por arquitecturas más avanzadas como Transformers, que manejan contexto de manera más eficiente mediante mecanismos de atención.
            
            \item \textbf{Arquitecturas de Deep Learning basadas en Transformers}: Estas nuevas arquitecturas revolucionaron el Procesamiento de Lenguaje Natural al superar las limitaciones de las arquitecturas de Redes Neuronales anteriores, gracias a su capacidad para procesar secuencias en paralelo y capturar relaciones a largo plazo mediante el mecanismo de atención. 
            Introducidos en el paper "Attention Is All You Need" \cite{vaswani2023attentionneed}, los Transformers utilizan mecanismos de self-attention para asignar pesos a cada palabra en función de su relevancia dentro del contexto, lo que permite una comprensión más profunda del significado del texto. 
            Modelos como BERT (Bidirectional Encoder Representations from Transformers \cite{devlin2019bertpretrainingdeepbidirectional}), GPT (Generative Pre-trained Transformer \cite{yenduri2023generativepretrainedtransformercomprehensive}) y T5 (Text-to-Text Transfer Transformer \cite{raffel2023exploringlimitstransferlearning}) lograron en su momento avances significativos en tareas como traducción automática, generación de texto, análisis de sentimientos y respuesta a preguntas. 
            Estas arquitecturas destacan por su capacidad de preentrenamiento en grandes corpus de datos y posterior ajuste fino en tareas específicas, lo que ha permitido obtener resultados de vanguardia en múltiples aplicaciones de PLN. 
            Tomando como base estas arquitecturas, han aparecido en los últimos años los Grandes Modelos del Lenguaje (LLMs), dando origen al desarrollo de la llamada Inteligencia Artificial Generativa.
        \end{itemize}

\end{itemize}

Estas técnicas han permitido avances en aplicaciones como asistentes virtuales, generación de texto automatizada y motores de búsqueda, optimizando la interacción entre humanos y sistemas computacionales.






\section{Inteligencia Artificial Generativa}

La inteligencia artificial generativa (IA generativa) es un campo de la inteligencia artificial que se centra en la creación de contenido nuevo y original a partir de datos existentes. 
A través del uso de modelos de aprendizaje profundo y técnicas avanzadas de procesamiento de datos, la IA generativa es capaz de generar texto, imágenes, audio, video y otros tipos de contenido de manera autónoma y coherente.  
El desarrollo de la IA generativa ha sido impulsado por la evolución de arquitecturas de redes neuronales como las redes generativas adversarias (GANs) \cite{goodfellow2014generativeadversarialnetworks}, los modelos basados en transformers \cite{vaswani2023attentionneed} y los modelos de difusión \cite{yang2024diffusionmodelscomprehensivesurvey}. 
Estas tecnologías han permitido la generación de contenido con una calidad cada vez mayor, logrando resultados que pueden ser indistinguibles de los creados por humanos.  

Su impacto ha sido significativo en múltiples sectores. 
En la industria creativa, ha transformado el diseño gráfico, la producción audiovisual y la generación de música, facilitando la automatización de tareas y ampliando las posibilidades de expresión artística \cite{iagengraphicdesign}. 
En el ámbito de la ciencia y la tecnología, se ha utilizado para la síntesis de datos, el descubrimiento de fármacos y la optimización de algoritmos \cite{ding2024risegenerativeartificialintelligence}. 
En el sector empresarial, la IA generativa ha revolucionado el marketing, la generación de contenido automatizado y la personalización de experiencias para los usuarios \cite{genaibusiness}.  

A medida que la tecnología continúa evolucionando, la IA generativa se enfrenta a desafíos éticos y regulatorios \cite{Hagendorff_2024}, incluyendo cuestiones sobre derechos de autor, desinformación y uso responsable. Sin embargo, su potencial sigue expandiéndose, con nuevos avances que permiten aplicaciones aún más sofisticadas y accesibles para diversos campos.


Los modelos de IA Generativa emplean diferentes técnicas de aprendizaje profundo para generar contenido, optimizando la calidad, coherencia y realismo de los resultados.
Las arquitecturas de redes neuronales basadas en transformers, introducidos en el artículo "Attention Is All You Need" (Vaswani et al., 2017) \cite{vaswani2023attentionneed} son la base de muchos de los modelos de IA generativa actuales, incluyendo los modelos de la familia GPT, Gemini de Google y Claude de Anthropic. 
Su principal ventaja radica en su capacidad para procesar y generar texto, imágenes, código y otros tipos de datos de manera eficiente y con una gran coherencia contextual.

Los transformers emplean un mecanismo llamado self-attention, que les permite asignar pesos a diferentes partes de la secuencia de entrada para comprender mejor las relaciones entre palabras o elementos. el flujo de esta arquitectura funciona de la siguiente manera:

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/transformer_arquitecture.png}
	\caption{Arquitectura transformer}
	\label{fig:context-anoni1}
\end{figure}


\begin{itemize}
    \item \textbf{Codificación de entrada (Embedding Layer)}: Convierte los datos de entrada (por ejemplo, texto o imágenes) en vectores numéricos que representan cada elemento en un espacio multidimensional. En el caso del texto, se utiliza un embedding de palabras, que transforma cada token en un vector denso de alta dimensión.
    \item \textbf{Mecanismo de Self-Attention}: Permite que el modelo asigne importancia a diferentes palabras dentro de una oración o documento. Se basa en tres matrices: Query (Q), Key (K) y Value (V), que permiten calcular la relevancia de cada palabra con respecto a las demás. Se utiliza una función de Softmax para asignar pesos a cada conexión, ajustando la atención que se le da a diferentes partes del contexto.
    \item \textbf{Capas de Normalización y Feed-Forward}: Cada capa de atención es seguida por una red neuronal completamente conectada (feed-forward network) y una normalización de los datos para mejorar la estabilidad del entrenamiento.
    \item \textbf{Mecanismo de Positional Encoding}: Dado que los transformers no tienen memoria secuencial como las redes recurrentes, utilizan codificaciones posicionales para mantener el orden de los elementos en la secuencia. Estas codificaciones se agregan a los embeddings de entrada y permiten que el modelo distinga la estructura del texto.
    \item \textbf{Capa de Decodificación (para modelos generativos)}: En los modelos generativos, la salida del transformer se pasa a una capa de decodificación que genera una secuencia de salida basada en los tokens previos. Utiliza un enfoque autoregresivo, donde se genera un token a la vez y se retroalimenta al modelo para predecir el siguiente.
\end{itemize}

La inteligencia artificial generativa abarca diversas aplicaciones, como la creación de imágenes, texto, audio, video y modelos 3D. 
Sin embargo, debido a la naturaleza de este proyecto, nos centraremos principalmente en los grandes modelos de lenguaje, que son capaces de generar texto coherente y contextual a partir de indicaciones, transformando la interacción con la tecnología y facilitando la creación de contenido escrito de manera automática.

\section{Grandes Modelos del Lenguaje}

Los Grandes Modelos del Lenguage (LLMs) han generado gran atención debido a su sólido rendimiento en una amplia gama de tareas de lenguaje natural desde el lanzamiento de ChatGPT en noviembre de 2022. 
La capacidad de comprensión y generación de lenguaje de propósito general de los LLMs se adquiere mediante el entrenamiento de miles de millones de parámetros del modelo con grandes volúmenes de datos textuales. 
Aunque el área de investigación de los LLMs es muy reciente, está evolucionando rápidamente.

\subsection{Primeros modelos del lenguaje}
Con el auge de los Transformers, empezaron a surgir nuevos modelos cada vez mas complejos, incluyendo cada vez un volumen de parámetros mayor.
BERT en 2018 marcó un punto de inflexión al introducir un preentrenamiento bidireccional basado en masked language modeling (MLM), mejorando la comprensión contextual en diversas tareas. 
Posteriormente, GPT-2 en 2019 \cite{radford2019language} se centró en la generación de texto autoregresiva, destacando por su coherencia y fluidez, aunque con preocupaciones sobre su posible uso indebido. 
En paralelo, T5 en 2019 \cite{raffel2023exploringlimitstransferlearning} propuso un enfoque texto a texto, permitiendo abordar múltiples tareas con una arquitectura unificada. 
La llegada de GPT-3 en 2020 \cite{brown2020languagemodelsfewshotlearners} con 175B parámetros supuso un avance significativo en few-shot learning, posibilitando la generación de contenido sin necesidad de ajuste fino específico. 
Finalmente, LaMDA en 2021 \cite{thoppilan2022lamdalanguagemodelsdialog} optimizó la interacción en diálogos abiertos, mejorando la coherencia en conversaciones de largo alcance y consolidando los avances en modelos conversacionales. 


\subsection{ChatGPT}
El lanzamiento de ChatGPT por OpenAI en noviembre de 2022 marcó un hito significativo en la evolución de los modelos de lenguaje, especialmente en aplicaciones de interacción conversacional \cite{Liu_2023}. 
ChatGPT es una versión mejorada de GPT-3, que fue afinada específicamente para mejorar la calidad de los diálogos y la coherencia en respuestas a preguntas abiertas y conversaciones continuas. 
ChatGPT se entrenó utilizando un enfoque de Reinforcement Learning from Human Feedback (RLHF), donde los humanos proporcionaron retroalimentación activa sobre las respuestas generadas, lo que permitió al modelo mejorar su capacidad de ofrecer respuestas más relevantes, precisas y menos propensas a producir información errónea o sesgada.


\subsection{LLaMA}
LLaMA (Large Language Model Meta AI), lanzado por Meta (anteriormente Facebook) en 2023, se presentó como un competidor directo de GPT-3 \cite{touvron2023llamaopenefficientfoundation}. 
LLaMA se desarrolló con un enfoque en ser un modelo de gran escala, pero de mayor accesibilidad para la investigación académica y la comunidad de código abierto. 
LLaMA está diseñado para ser más eficiente y eficaz, permitiendo a los investigadores utilizar modelos de lenguaje a gran escala sin la necesidad de grandes cantidades de infraestructura.

Características clave de LLaMA:
\begin{itemize}
    \item \textbf{Accesibilidad}: Meta liberó versiones de LLaMA con diferentes tamaños de parámetros, haciendo posible que se usaran en equipos de investigación más pequeños.
    \item \textbf{Desempeño a gran escala}: A pesar de que LLaMA no es tan grande como GPT-4, mostró un rendimiento comparable en tareas de procesamiento de lenguaje natural (PLN).
    \item \textbf{Eficiencia}: El modelo fue diseñado para ser más eficiente en términos de uso de recursos, lo que lo hacía más accesible para investigadores en lugar de depender de poderosas infraestructuras.
\end{itemize}

LLaMA ayudó a democratizar el acceso a modelos de lenguaje de gran escala, permitiendo que más organizaciones y académicos participaran en la investigación y desarrollo de modelos avanzados.


\subsection{GPT-4}

En marzo de 2023, OpenAI lanzó GPT-4, un modelo significativamente más avanzado que GPT-3 y ChatGPT \cite{openai2024gpt4technicalreport}. 
Con una capacidad de procesar entradas multimodales (texto e imágenes), GPT-4 permitió nuevas aplicaciones en áreas como análisis de contenido visual y descripciones detalladas. 
Además, mostró mejoras significativas en razonamiento complejo, en la resolución de problemas matemáticos avanzados y en la comprensión de contextos más largos.

sus principales características son:

\begin{itemize}
    \item Multimodalidad: GPT-4 podía procesar imágenes junto con texto, lo que le permitió realizar tareas como la interpretación de gráficos, la descripción de imágenes y la creación de contenido visual a partir de texto.
    \item Razonamiento mejorado: El modelo mostró un mejor desempeño en tareas que requieren razonamiento lógico y la capacidad de resolver problemas complejos.
    \item Mayor capacidad de manejo de contexto: A diferencia de GPT-3, GPT-4 pudo gestionar contextos más largos, lo que resultó en una mayor coherencia durante conversaciones extensas.    
\end{itemize}


\subsection{DeepSeek}
DeepSeek es un modelo propuesto en 2023, cuyo enfoque se centra en la búsqueda semántica profunda y la comprensión avanzada de consultas complejas \cite{deepseekai2024deepseekllmscalingopensource}. 
Este modelo es particularmente útil en sistemas de búsqueda y recuperación de información, donde se requiere un entendimiento más allá de las coincidencias exactas de palabras clave, permitiendo la interpretación semántica de las consultas.

Características principales de DeepSeek:
\begin{itemize}
    \item \textbf{Búsqueda semántica avanzada}: DeepSeek está optimizado para interpretar y responder a consultas complejas, mejorando la precisión de las búsquedas más allá de simples coincidencias de términos.
    \item \textbf{Comprensión del contexto}: Utiliza técnicas de procesamiento de lenguaje natural para interpretar el significado detrás de las consultas, lo que permite ofrecer resultados más relevantes y contextualizados.
    \item \textbf{Aplicaciones en la búsqueda web y bases de datos}: DeepSeek ha demostrado ser particularmente útil en la mejora de los motores de búsqueda, brindando una mejor experiencia al usuario mediante la entrega de resultados más precisos y personalizados.
\end{itemize}

DeepSeek es un ejemplo del progreso continuo hacia el entendimiento más profundo del lenguaje y su aplicación en la búsqueda semántica, un área clave en la IA.


\subsection{LLaMA 2}

En julio de 2023, Meta AI lanzó LLaMA 2, la segunda generación de su familia de modelos de lenguaje de código abierto \cite{touvron2023llama2openfoundation}. 
Este lanzamiento representó un avance significativo en la democratización de modelos de lenguaje de gran escala, proporcionando a la comunidad investigadora y a empresas un acceso sin restricciones a modelos altamente capaces para diversas aplicaciones.

LLaMA 2 conserva la arquitectura basada en transformers de su predecesor, pero con mejoras en eficiencia, entrenamiento y alineación. 
Se lanzó en tres versiones principales, diferenciadas por su tamaño en parámetros: 7B (7 mil millones de parámetros), 13B y 65B, permitiendo su implementación en distintos entornos según las necesidades de hardware y rendimiento.

Las mejoras clave en LLaMA 2 incluyen:
\begin{itemize}
    \item \textbf{Entrenamiento con un corpus de datos significativamente más amplio}, lo que mejoró su capacidad de generalización.
    \item \textbf{Optimización de eficiencia computacional}, logrando una mejor relación entre calidad de respuesta y costos computacionales.
    \item \textbf{Mejor alineación y reducción de sesgos}, gracias a un refinamiento en su entrenamiento mediante aprendizaje por refuerzo con retroalimentación humana (RLHF).
    \item \textbf{Mayor estabilidad en generación de texto}, proporcionando respuestas más coherentes y precisas en conversaciones prolongadas.
\end{itemize}

En términos de rendimiento, LLaMA 2-13B demostró ser comparable a modelos comerciales más grandes como GPT-3.5, mientras que la versión 65B se acercó en capacidad a GPT-4 en ciertas tareas de procesamiento de lenguaje natural.

\subsection{Mistral}

En septiembre de 2023, la empresa Mistral AI lanzó Mistral 7B, un modelo de lenguaje de código abierto que destacó por su eficiencia y rendimiento optimizado, superando a otros modelos de tamaño similar \cite{jiang2023mistral7b}. 
Con un enfoque en alta calidad y accesibilidad, Mistral 7B se convirtió rápidamente en una de las opciones más utilizadas en la comunidad de IA de código abierto.

Mistral 7B se diseñó con una arquitectura optimizada basada en transformers densos, pero con mejoras específicas:

\begin{itemize}
    \item \textbf{Uso de bloques de atención eficiente}: Lo que permitió un mejor manejo del contexto y una mayor capacidad de generalización.
    \item \textbf{Reducción del tamaño del modelo sin sacrificar el rendimiento}: Optimizando la relación entre calidad y consumo de recursos.
    \item \textbf{Mejor respuesta en tareas de razonamiento y generación de texto}: Alcanzando un rendimiento comparable a modelos más grandes como LLaMA 2-13B.
    \item \textbf{Diseño modular y altamente escalable}: Facilitando la integración en diversas aplicaciones comerciales y de investigación.
\end{itemize}

En comparación con modelos de tamaño similar, Mistral 7B mostró superioridad en generación de código, razonamiento matemático y comprensión de instrucciones, convirtiéndolo en una opción viable frente a modelos más grandes como LLaMA 2-13B y Falcon 40B.
Si bien Mistral 7B es más pequeño en parámetros que modelos como DeepSeek V2 (16B) o LLaMA 2-13B, su eficiencia le permitió superar a ambos en múltiples tareas. Su rendimiento fue similar al de modelos cerrados como GPT-3.5, pero con la ventaja de ser de código abierto y optimizado para ejecución en hardware menos costoso.


\subsection{LLaMA 3}

LLaMA 3 fue lanzado por Meta en marzo de 2024, mejorando la versión anterior de LLaMA con capacidades avanzadas y optimizaciones aún más eficientes \cite{grattafiori2024llama3herdmodels}. 
Este modelo continúa con la tendencia de facilitar el acceso a modelos de gran escala mientras aumenta el rendimiento en tareas de procesamiento del lenguaje natural y la generación de texto.

Principales avances en LLaMA 3:
\begin{itemize}
    \item \textbf{Mejora en eficiencia computacional}: LLaMA 3 se diseñó para ser aún más eficiente que sus predecesores, permitiendo que más instituciones, especialmente académicas y de investigación, puedan usarlo para diversos proyectos.
    \item \textbf{Mayor capacidad de razonamiento}: LLaMA 3 incluye avances en el razonamiento lógico y la resolución de problemas complejos, lo que lo hace más competitivo frente a modelos como GPT-4.
    \item \textbf{Entrenamiento más accesible}: Meta optimizó aún más los procesos de entrenamiento, lo que permite a los investigadores entrenar versiones del modelo en un rango más amplio de infraestructuras.
\end{itemize}

LLaMA 3 sigue siendo una opción accesible para la investigación en el ámbito de los LLMs, brindando una combinación de potencia y eficiencia.


\subsection{deepseek V2}

En 2024, DeepSeek AI presentó DeepSeek V2, una versión mejorada de su modelo de búsqueda semántica \cite{deepseekai2024deepseekv2strongeconomicalefficient}. 
Esta versión avanzó notablemente en la precisión y adaptabilidad de los resultados, especialmente al integrar capacidades multimodales y mejorar la generación de respuestas contextualizadas en tiempo real.

Características de DeepSeek V2:
\begin{itemize}
    \item \textbf{Mejor capacidad de comprensión multimodal}: DeepSeek v2 ahora es capaz de interpretar no solo texto, sino también imágenes, audio y otros tipos de datos, lo que le permite ofrecer respuestas aún más completas.
    \item \textbf{Generación de respuestas en tiempo real}: El modelo mejora la interacción con los usuarios al generar respuestas más rápidas y adaptadas a consultas complejas en tiempo real.
    \item \textbf{Mayor precisión en tareas de búsqueda y análisis}: La nueva versión presenta una mejora significativa en la precisión de la recuperación de información y la generación de respuestas relevantes, ajustadas al contexto.
\end{itemize}

DeepSeek v2 se destacó como una herramienta esencial para sistemas de búsqueda avanzados y la recuperación semántica de información, adaptándose a las necesidades de los usuarios de manera más efectiva.


\subsection{GPT-4o}

En mayo de 2024, OpenAI presentó GPT-4o (omni), una nueva iteración de su serie de modelos GPT, con mejoras significativas en velocidad, eficiencia y capacidades multimodales \cite{openai2024gpt4ocard}. 
A diferencia de su predecesor GPT-4, que dependía de una arquitectura con distintos expertos (Mixture of Experts, MoE) y procesamiento separado para texto, imágenes y audio, GPT-4o adopta un enfoque nativamente multimodal, permitiendo procesar y generar texto, audio e imágenes con latencias mucho más bajas.

Uno de los aspectos más innovadores de GPT-4o es su capacidad para manejar múltiples modalidades de entrada y salida en tiempo real. 
A diferencia de modelos anteriores que requerían módulos separados para audio y visión, GPT-4o emplea un solo modelo unificado, optimizado para la inferencia eficiente en diversas tareas. 
Esto permite interacciones más naturales y rápidas, especialmente en aplicaciones conversacionales y de asistencia virtual.

Además, OpenAI mejoró la velocidad de respuesta del modelo, alcanzando tiempos de procesamiento hasta 2× más rápidos que GPT-4-turbo, con una reducción considerable en los costos computacionales. Esto ha permitido desplegar GPT-4o como la opción por defecto en la API de OpenAI y en ChatGPT, democratizando el acceso a un modelo de alto rendimiento sin restricciones significativas.

GPT-4o es el primer modelo de OpenAI diseñado desde cero para integrar texto, audio e imágenes de manera fluida. 
En términos de procesamiento de audio, es capaz de reconocer y generar voz en tiempo real, permitiendo conversaciones fluidas sin la latencia que caracterizaba a versiones previas. 
En el ámbito visual, mejora la comprensión de imágenes y videos, ofreciendo capacidades avanzadas en reconocimiento de objetos, descripción de escenas y generación de contenido visual.

En tareas de procesamiento de lenguaje natural, GPT-4o supera a sus predecesores en comprensión, generación de texto, traducción automática y razonamiento contextual. 
Su capacidad para manejar consultas más complejas, junto con una mayor coherencia en respuestas largas, lo posiciona como uno de los modelos más potentes en la actualidad. 
También ha demostrado mejoras en la reducción de alucinaciones y en la precisión factual, aunque sigue enfrentando desafíos en ciertas tareas de razonamiento avanzado.


Gracias a su rapidez y capacidades mejoradas, GPT-4o ha sido implementado en una variedad de aplicaciones, desde asistentes virtuales avanzados hasta herramientas de productividad basadas en IA. 
Su integración en la aplicación web ChatGPT ha permitido a los usuarios experimentar una IA más interactiva y versátil, con respuestas más rápidas y contextualizadas. 
Además, la reducción en los costos de inferencia ha facilitado su adopción en productos comerciales y empresariales.

Con GPT-4o, OpenAI ha dado un paso importante hacia una inteligencia artificial más accesible, interactiva y eficiente, consolidando su liderazgo en el desarrollo de modelos multimodales de alto rendimiento.


\subsection{GPT-4o mini}

En junio de 2024, OpenAI lanzó GPT-4o Mini, una versión optimizada y más ligera de GPT-4o, diseñada para ofrecer un equilibrio entre rendimiento, eficiencia y accesibilidad. 
Este modelo mantiene muchas de las capacidades avanzadas de su predecesor, pero con un enfoque en menor consumo computacional, lo que lo hace ideal para aplicaciones en dispositivos con recursos limitados y para despliegues comerciales a gran escala.

A diferencia del modelo completo GPT-4o, que está diseñado para tareas de alta demanda en procesamiento de texto, imagen y audio en tiempo real, GPT-4o Mini reduce la complejidad computacional, permitiendo tiempos de respuesta aún más rápidos con un menor costo energético. 
OpenAI ha optimizado este modelo para ofrecer rendimiento competitivo en tareas de Procesamiento de Lenguaje natural, sin comprometer significativamente la calidad de las respuestas.

Esta versión sigue el principio de modelos nativamente multimodales, lo que significa que puede manejar texto, imágenes y audio de manera integrada, aunque con menor capacidad de procesamiento en tareas extremadamente complejas. 
Esto lo hace ideal para asistentes virtuales, chatbots, herramientas de productividad y aplicaciones móviles, donde la latencia y el consumo de recursos son factores clave.

Si bien GPT-4o Mini conserva muchas de las mejoras de su versión completa, presenta algunas diferencias en términos de profundidad de razonamiento y capacidad de generación en tareas extensas. 
Entre sus características principales se destacan:
\begin{itemize}
    \item \textbf{Mayor eficiencia en dispositivos con menos recursos}: facilitando su implementación en plataformas con hardware limitado.
    \item \textbf{Rendimiento optimizado para tareas de conversación y asistencia virtual}: con respuestas más rápidas y coherentes en diálogos cotidianos.
    \item \textbf{Menor latencia en comparación con modelos más grandes}: lo que lo hace ideal para aplicaciones en tiempo real.
    \item \textbf{Capacidades multimodales básicas}: aunque con menor profundidad en la comprensión y generación de contenido visual y auditivo en comparación con GPT-4o completo.
\end{itemize}

Gracias a su balance entre velocidad y capacidad, GPT-4o Mini ha sido adoptado en múltiples industrias, desde atención al cliente automatizada hasta educación y asistentes personales. 
Empresas tecnológicas han comenzado a integrarlo en productos de bajo consumo computacional, asegurando interacciones de IA más fluidas y económicas.

Con el lanzamiento de GPT-4o Mini, OpenAI ha reforzado su enfoque en modelos accesibles y eficientes, ofreciendo una opción poderosa para usuarios y desarrolladores que requieren inteligencia artificial de alto nivel sin los costos computacionales de modelos más grandes. 


\subsection{Claude 3.5 Sonnet}

En julio de 2024, Anthropic lanzó Claude 3.5 Sonnet, una evolución significativa dentro de la serie Claude 3, mejorando notablemente el rendimiento en razonamiento complejo, generación de texto y eficiencia computacional. 
Este modelo se posiciona como un competidor directo de GPT-4o, ofreciendo capacidades avanzadas en comprensión contextual y generación fluida de contenido.

Claude 3.5 Sonnet mantiene la arquitectura basada en transformers, pero con optimización en la gestión de memoria y eficiencia en la atención. 
Se ha reportado una notable mejora en la velocidad de procesamiento y en la capacidad para manejar contextos extensos sin degradación del rendimiento. 
Además, se han implementado técnicas avanzadas para reducir alucinaciones y mejorar la precisión factual.

En comparación con sus predecesores, Claude 3.5 Sonnet sobresale en:

\begin{itemize}
    \item Mayor precisión en tareas de razonamiento lógico y matemático.
    \item Mejora en la comprensión de documentos extensos y consultas complejas.
    \item Optimización en generación de código, con respuestas más estructuradas y precisas.
    \item Reducción de sesgos y alineación mejorada con instrucciones del usuario.
\end{itemize}

Si bien Claude 3.5 Sonnet aún no ofrece capacidades multimodales tan avanzadas como GPT-4o, su procesamiento de texto sigue siendo de alto nivel, lo que lo hace ideal para tareas como asistencia en redacción, generación de informes, investigación académica y programación.

Comparado con modelos anteriores de Anthropic, Claude 3.5 Sonnet es más rápido y preciso, acercándose en rendimiento a modelos de última generación de GPT y DeepSeek.

Debido a su balance entre rendimiento y eficiencia, Claude 3.5 Sonnet ha sido adoptado en entornos empresariales, plataformas de asistencia virtual y generación de contenido automatizado. 
Además, su lanzamiento ha fortalecido la posición de Anthropic en la competencia por modelos de IA de alto rendimiento, consolidándose como una de las opciones más avanzadas para usuarios que requieren modelos precisos y eficientes. 

\subsection{DeppSeek V3}

En enero de 2025, DeepSeek AI lanzó DeepSeek V3, su modelo más avanzado hasta la fecha, que marca una evolución significativa respecto a las versiones anteriores. 
DeepSeek V3 está diseñado para ofrecer capacidades de razonamiento avanzado, generación de contenido multimodal y aprendizaje más rápido, con un enfoque en mejorar la precisión en tareas complejas y optimizar el rendimiento en plataformas comerciales.

DeepSeek V3 utiliza una arquitectura de transformers de última generación, mejorando los mecanismos de atención y la gestión de memoria para trabajar con secuencias más largas y complejas. 
Su principal avance radica en la optimización de la interacción multimodal, donde puede procesar texto, imágenes y audio de manera más eficiente, permitiendo un rendimiento fluido en tareas que combinan estas modalidades.

Además, DeepSeek V3 ha mejorado el razonamiento lógico y la generación de código, lo que lo hace particularmente adecuado para aplicaciones en áreas de desarrollo de software y análisis técnico. 
Comparado con versiones anteriores, su capacidad de generalización y alineación con las intenciones del usuario es más precisa, lo que reduce significativamente los errores de interpretación.

Aunque DeepSeek V2 ya ofrecía un rendimiento sólido en múltiples dominios, DeepSeek V3 introduce mejoras importantes:

\begin{itemize}
    \item \textbf{Mejor razonamiento contextual y manejo de tareas complejas}: El modelo ha optimizado su capacidad para resolver problemas que requieren múltiples pasos de razonamiento.
    \item \textbf{Generación de contenido más coherente y preciso}: Especialmente en tareas de escritura técnica y generación de código.
    \item \textbf{Mayor capacidad de alineación con instrucciones específicas del usuario}: Lo que lo hace más eficiente en tareas de personalización.
    \item \textbf{Mayor capacidad multimodal}: Con mejoras en la interpretación y creación de imágenes y audio a partir de texto, aunque sigue siendo un modelo más centrado en texto en comparación con modelos completamente multimodales como GPT-4o.
\end{itemize}

Gracias a su rendimiento mejorado, DeepSeek V3 ha sido adoptado en sectores que requieren precisión avanzada, como la generación de código, análisis de datos, desarrollo de software y asistencia en investigación. 
Su integración con plataformas de desarrollo de software automatizado ha permitido a empresas optimizar la creación de soluciones tecnológicas más rápidamente.

Además, DeepSeek V3 ha ganado popularidad en herramientas de productividad y asistentes virtuales, gracias a su capacidad para mantener diálogos más coherentes y generar respuestas más precisas en conversaciones prolongadas. 
Con este lanzamiento, DeepSeek se consolida como un jugador clave en el ámbito de la inteligencia artificial, ofreciendo soluciones de alto rendimiento para una variedad de aplicaciones comerciales. 


\section{Prompt Engineering}

El Prompt Engineering (ingeniería de prompts) ha surgido como una técnica clave para mejorar las capacidades de los Grandes Modelos de Lenguaje preentrenados \cite{sahoo2025systematicsurveypromptengineering}. 
Consiste en diseñar de manera estratégica instrucciones específicas para tareas, conocidas como prompts, que guían la salida del modelo sin modificar sus parámetros. 
La importancia del prompt engineering es especialmente evidente en su impacto transformador sobre la adaptabilidad de estos modelos. 
Al ofrecer un mecanismo para ajustar los resultados del modelo mediante instrucciones cuidadosamente elaboradas, el prompt engineering permite a estos modelos sobresalir en una amplia gama de tareas y dominios. 
Esta adaptabilidad se diferencia de los paradigmas tradicionales, donde a menudo se requiere un reentrenamiento del modelo o un ajuste fino extenso para lograr un rendimiento específico para una tarea.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/prompt_engineering.png}
	\caption{Flujo del prompt engineering}
	\label{fig:context-anoni1}
\end{figure}


A continuación se describen algunas de las técnicas de Prompt Engineering mas comunes.

\subsection{Zero-Shot Prompting}
El zero-shot prompting representa un cambio de paradigma en la utilización de Grandes Modelos del Lenguaje \cite{radford2019}. 

Esta técnica elimina la necesidad de grandes volúmenes de datos de entrenamiento, basándose en prompts diseñados cuidadosamente para guiar al modelo en la resolución de tareas novedosas. 
Específicamente, el modelo recibe una descripción de la tarea dentro del prompt, pero no dispone de datos etiquetados para entrenarse en mapeos específicos de entrada-salida. 

En su lugar, el modelo aprovecha su conocimiento preexistente para generar predicciones basadas en el prompt proporcionado para la nueva tarea.

\subsection{Few-Shot Prompting}
El few-shot prompting proporciona a los modelos algunos ejemplos de entrada-salida para inducir la comprensión de una tarea específica \cite{brown2020language}, a diferencia del zero-shot prompting, donde no se suministran ejemplos. 
Incluir incluso unos pocos ejemplos de alta calidad ha demostrado mejorar el rendimiento del modelo en tareas complejas en comparación con la ausencia de estos. 

Sin embargo, el few-shot prompting requiere tokens adicionales para incorporar los ejemplos, lo que puede encarecer el coste de la tarea en entradas de texto más extensas. 
Además, la selección y composición de los ejemplos dentro del prompt pueden influir significativamente en el comportamiento del modelo, y sesgos como la preferencia por palabras frecuentes pueden seguir afectando los resultados. 

Aunque el few-shot prompting mejora las capacidades en tareas complejas, una ingeniería de prompts cuidadosa es esencial para alcanzar un rendimiento óptimo y mitigar sesgos no intencionados.

\subsection{Chain-of-Thought Prompting}
Los Grandes Modelos del Lenguaje suelen enfrentar dificultades en tareas de razonamiento complejo, lo que limita su potencial. 
Con el objetivo de abordar esta limitación, se desarrolló el concepto de Chain-of-Thought (CoT) prompting \cite{wei2023chainofthoughtpromptingelicitsreasoning}, una técnica que permite estructurar prompts de manera que faciliten procesos de razonamiento coherentes y paso a paso. 

La principal contribución de este enfoque radica en su capacidad para inducir respuestas más estructuradas y reflexivas en comparación con los prompts tradicionales.
A través de una serie de experimentos, se demostró que el CoT prompting guía a los modelos a seguir una cadena lógica de razonamiento, lo que resulta en respuestas que reflejan una comprensión más profunda de los prompts dados. 
Por ejemplo, en un problema matemático de múltiples pasos, el prompt incluiría tanto el proceso de razonamiento como la respuesta final, imitando la forma en que los humanos descomponen los problemas en pasos intermedios lógicos.


\subsection{Instruction Following}

La técnica de instruction following (seguimiento de instrucciones) se centra en diseñar prompts que especifiquen explícitamente las instrucciones sobre cómo el modelo debe comportarse durante el proceso de inferencia \cite{lou2024largelanguagemodelinstruction}. 
Esto incluye detalles como el formato de la respuesta esperada, el estilo de escritura, e incluso las restricciones de tiempo para la generación. 
La capacidad de un modelo para seguir instrucciones con precisión depende tanto de la claridad del prompt como de la estructura subyacente del modelo.

Esta técnica es especialmente útil en tareas de generación de texto controlada, como la redacción de resúmenes, traducción automática o generación de contenido específico en un formato predeterminado. 
La implementación efectiva de instruction following depende de la habilidad del modelo para interpretar y ejecutar las instrucciones de forma coherente, lo que requiere un equilibrio entre la precisión del modelo y la claridad del prompt.

\subsection{Limitaciones de Promt Engineering}
El prompt engineering ha revolucionado la forma en que los modelos preentrenados son utilizados en diversas aplicaciones, permitiendo un alto grado de adaptabilidad sin la necesidad de reentrenamiento. 
Las técnicas presentadas representan solo una fracción del potencial que ofrece el campo. 

Sin embargo, aunque estas estrategias optimizan la generación de respuestas, los modelos siguen enfrentando limitaciones cuando se requiere información actualizada o específica que no está presente en sus datos de entrenamiento.
Para abordar esta limitación, surge el enfoque de Retrieval-Augmented Generation (RAG), que combina la capacidad generativa de los modelos con sistemas de recuperación de información en tiempo real.


\section{Retrieval Augmented Generation (RAG)}

Los Grandes Modelos del Lenguaje han logrado un éxito notable; sin embargo, aún presentan limitaciones significativas, especialmente en tareas específicas de dominio o con alta demanda de conocimiento. 
En particular, generan alucinaciones al procesar consultas que exceden sus datos de entrenamiento o requieren información actualizada.

Para abordar estos desafíos, se desarrolla la técnica Retrieval Augmented Generation (Generación Aumentada por Recuperación, o RAG) \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, que optimiza el desempeño de los LLMs mediante la recuperación de fragmentos de documentos relevantes desde bases de conocimiento externas, utilizando cálculos de similitud semántica. 
Al referenciar información externa, RAG mitiga la generación de contenido incorrecto o alucinado. 
Su integración con los LLMs ha impulsado su adopción generalizada, consolidando a RAG como una tecnología clave en la evolución de los chatbots y mejorando la aplicabilidad de los LLMs en entornos reales.

\newpage
El funcionamiento de un sistema RAG se basa en la combinación de recuperación de información y generación de texto, optimizando la precisión y actualidad de las respuestas mediante LLMs. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/rag.png}
	\caption{flujo de RAG}
	\label{fig:context-anoni1}
\end{figure}


\begin{itemize}
    \item En la fase de \textbf{recuperación}, la consulta del usuario se convierte en un vector mediante un modelo de embeddings y se compara con una base de datos vectorizada, que almacena documentos representados como vectores en un espacio de alta dimensión.
    \item Estas bases de datos permiten realizar recuperación de conocimiento relevante mediante algoritmos de similitud, reduciendo el costo computacional en la recuperación de información relevante.
    \item Los fragmentos más cercanos semánticamente a la consulta son recuperados y concatenados con el prompt, ingresando a la fase de \textbf{generación}, donde un LLM procesa la información contextualizada y genera una respuesta fundamentada.
    \item Finalmente, en la fase de \textbf{aumento}, se pueden aplicar diferentes técnicas para optimizar la coherencia y factualidad de la generación.
\end{itemize}

Este enfoque permite superar limitaciones de los LLMs, como la generación de contenido alucinatorio o el acceso a información desactualizada, consolidando a RAG como una arquitectura clave en la integración de modelos de lenguaje con bases de conocimiento dinámicas y eficientes.

\subsection{Clasificación del RAG}

El paradigma de investigación en RAG evoluciona continuamente y se clasifica en tres bloques:
\subsubsection{Naive RAG}
Representa la metodología inicial de RAG. Se basa en un esquema "Retrieve-Read", compuesto por tres fases: indexación, recuperación y generación \cite{ma2023queryrewritingretrievalaugmentedlarge}.
En la fase de indexación, se procesan datos en diversos formatos (PDF, HTML, Word, Markdown), normalizándolos a texto plano. 
Luego, se segmentan en fragmentos más pequeños para ajustarse a las limitaciones de contexto de los LLMs. 
Estos fragmentos son convertidos en vectores mediante un modelo de embeddings y almacenados en una base de datos vectorizada, lo que permite búsquedas eficientes en la fase de recuperación.
Durante la recuperación, la consulta del usuario se codifica en un vector utilizando el mismo modelo de embeddings. 
Posteriormente, se calculan similitudes semánticas entre la consulta y los fragmentos indexados, seleccionando los K fragmentos más relevantes para ampliar el contexto del prompt.
En la fase de generación, la consulta y los documentos recuperados se combinan en un prompt que el LLM procesa para generar una respuesta. 
Dependiendo de la tarea, el modelo puede combinar su conocimiento paramétrico con la información recuperada o limitarse exclusivamente a esta última. 
En diálogos continuos, el historial conversacional se incorpora para mejorar la coherencia en interacciones multi-turno.

\subsubsection{Advanced RAG}
Introduce mejoras específicas para superar las limitaciones de Naive RAG, con un enfoque en la optimización de la recuperación de información mediante estrategias pre-retrieval y post-retrieval \cite{ilin2023advanced}. 
Para abordar problemas de indexación, Advanced RAG refina sus técnicas mediante el uso de ventanas deslizantes, segmentación granular y metadatos, además de implementar métodos de optimización para mejorar la eficiencia del proceso de recuperación.

\begin{itemize}
    \item \textbf{Pre-Retrieval Process}: En esta etapa, se optimizan tanto la estructura de indexación como la consulta original del usuario. 
    La mejora en la indexación busca aumentar la calidad del contenido indexado mediante estrategias como mayor granularidad de datos, optimización de estructuras de índices, adición de metadatos, alineación de información y recuperación híbrida. 
    Por otro lado, la optimización de consultas tiene como objetivo hacer que la pregunta del usuario sea más clara y adecuada para la recuperación, utilizando técnicas como reescritura de consultas, transformación, expansión de consultas y otras estrategias.
    
    \item \textbf{Post-Retrieval Process}: Una vez recuperado el contexto relevante, es fundamental integrarlo de manera efectiva con la consulta original. 
    Las principales estrategias incluyen re-ranking y context compression. 
    El re-ranking reorganiza los fragmentos recuperados, priorizando la información más relevante en los extremos del prompt, una técnica utilizada en frameworks como LlamaIndex, LangChain y HayStack.
    Dado que alimentar todos los documentos recuperados a un LLM puede generar sobrecarga de información, diluyendo la relevancia del contenido, el proceso post-retrieval se centra en la selección de información clave, priorización de secciones críticas y reducción del contexto para mejorar la precisión y eficiencia en la generación de respuestas.
\end{itemize}

\subsubsection{Modular RAG}
Optimiza la adaptabilidad y precisión del paradigma RAG al introducir una arquitectura flexible con módulos especializados \cite{yu2023generateretrievelargelanguage}. 
Mejora la recuperación mediante técnicas avanzadas como búsqueda híbrida, multi-query retrieval y re-ranking inteligente. 
Sus nuevos módulos, como Search, Memory, Routing y Predict, permiten una recuperación más precisa y contextualizada, mientras que el Task Adapter personaliza la generación para diversas tareas.
A nivel estructural, Modular RAG reemplaza el flujo tradicional "Retrieve-Read" por esquemas dinámicos como Rewrite-Retrieve-Read, Generate-Read y Recite-Read, mejorando la integración de datos y reduciendo sesgos. 
Además, incorpora estrategias iterativas y autoajustables como Demonstrate-Search-Predict (DSP) y ITER-RETGEN, optimizando la interacción entre módulos.
Su flexibilidad permite la integración con técnicas de fine-tuning y reinforcement learning, optimizando tanto el retriever como el generador para mejorar la relevancia y coherencia de las respuestas.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/rag_types.png}
	\caption{Tipos de RAG}
	\label{fig:context-anoni1}
\end{figure}


\subsection{Técnicas de mejora de RAG}
Los sistemas RAG han ganado mucha atención debido a su capacidad para combinar el poder de los modelos generativos con la recuperación de información. 
Estos sistemas permiten a los modelos generar respuestas de alta calidad basadas en documentos relevantes recuperados de grandes bases de datos. 
Sin embargo, para mejorar aún más la precisión y la relevancia de las respuestas generadas, se han desarrollado técnicas avanzadas que optimizan tanto la recuperación como la generación de información. 
Entre estas técnicas destacan la búsqueda híbrida o hybrid search, el reordenamiento o reranking y el RAG contextual.


\subsubsection{Hybrid Search}
La Búsqueda Híbrida combina dos enfoques clave de recuperación de información: la búsqueda exacta basada en palabras clave y la búsqueda densa basada en embeddings \cite{hybridsearch}. 
Cada uno de estos métodos tiene ventajas y limitaciones, por lo que su combinación permite mejorar significativamente la calidad de la recuperación de documentos y, por ende, la precisión de las respuestas generadas por los modelos de lenguaje.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/hybrid_search.png}
	\caption{Flujo de búsqueda híbrida}
	\label{fig:context-anoni1}
\end{figure}

\begin{itemize}
    \item La \textbf{búsqueda exacta} utiliza palabras clave para localizar documentos mediante técnicas como el índice invertido, BM25 y TF-IDF, lo que permite respuestas rápidas y eficientes en grandes bases de datos. 
    Sin embargo, su limitación radica en la falta de comprensión semántica, lo que puede excluir documentos relevantes si no hay coincidencias exactas. 
    \item Por otro lado, \textbf{la búsqueda densa} convierte consultas y documentos en embeddings vectoriales utilizando LLMs, lo que permite medir similitudes semánticas y recuperar información más flexible. 
    Aunque más precisa en términos de significado, esta técnica requiere mayor capacidad computacional y modelos preentrenados, dependiendo de herramientas como bases de datos vectoriales para optimizar la búsqueda en grandes volúmenes de datos.
    \item La combinación de búsqueda exacta y búsqueda densa en un \textbf{modelo híbrido} aprovecha sus fortalezas complementarias, logrando un equilibrio entre eficiencia y precisión en la recuperación de documentos dentro de sistemas RAG.
    En este enfoque, la búsqueda exacta actúa como un filtro inicial rápido, identificando documentos potencialmente relevantes. Posteriormente, la búsqueda densa refina estos resultados, asegurando que la información seleccionada sea semánticamente relevante para la consulta.
    Para fusionar ambas metodologías, se pueden emplear estrategias como la fusión de puntajes, que normaliza y combina los resultados de cada método en un ranking unificado, o el filtrado progresivo, donde la búsqueda exacta reduce el conjunto de documentos antes de aplicar la búsqueda densa para su reordenamiento.
    Al integrar estas técnicas, los sistemas RAG mejoran la precisión de sus respuestas y reducen la recuperación de documentos irrelevantes, optimizando tanto la velocidad del proceso como la calidad de la información presentada al usuario.
\end{itemize}
En esencia, la búsqueda híbrida maximiza la eficiencia del proceso de recuperación al utilizar la rapidez de la búsqueda exacta junto con la capacidad semántica de la búsqueda densa. Este enfoque es particularmente útil en escenarios donde la consulta puede formularse de múltiples maneras y no siempre coincide textualmente con los documentos relevantes disponibles en la base de datos.

\subsubsection{Reranking}

El re-ranking es una técnica utilizada para mejorar la calidad de los documentos recuperados antes de ser utilizados en la generación de respuestas. 
Su propósito es reorganizar la lista de documentos obtenida en la fase de recuperación, priorizando aquellos que son más relevantes para la consulta del usuario.

En los sistemas RAG, la primera fase de búsqueda (ya sea exacta, densa o híbrida) devuelve un conjunto de documentos ordenados por su similitud con la consulta. 
Sin embargo, este orden inicial no siempre es el óptimo, ya que los métodos de recuperación pueden devolver documentos parcialmente relevantes o incluso irrelevantes. 
Aquí es donde entra el re-ranking, aplicando técnicas avanzadas para mejorar la selección final de documentos que serán procesados por el modelo generador.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/rerank.jpg}
	\caption{Esquema de Reranking}
	\label{fig:context-anoni1}
\end{figure}

Existen varios enfoques para realizar el re-ranking en sistemas RAG. Estos métodos pueden variar en complejidad y costo computacional, dependiendo del nivel de precisión requerido.

\begin{itemize}
    \item \textbf{Re-ranking basado en heurísticas}: Este enfoque utiliza reglas predefinidas y métricas estadísticas para reordenar los documentos recuperados sin necesidad de modelos de aprendizaje profundo. 
    Algunos métodos comunes incluyen algoritmos como TF-IDF o BM25 ajustado, que prioriza documentos donde los términos clave aparecen con mayor frecuencia y relevancia, y el criterio de posición de aparición, que favorece documentos donde las palabras clave se encuentran al inicio del texto. 
    También puede aplicarse un filtro basado en la longitud del documento, dando prioridad a textos más concisos si se asume que contienen información más relevante. 
    Aunque este método es rápido y eficiente, su principal limitación es que no tiene en cuenta el significado semántico de las palabras, lo que puede afectar la precisión de la recuperación en consultas más complejas.

    \item \textbf{Re-ranking con modelos de lenguaje}: En este método, se utilizan modelos del lenguaje para evaluar la relevancia semántica de los documentos con respecto a la consulta. 
    Modelos como BERT Cross-Encoder procesan cada consulta junto con un documento y generan una puntuación de relevancia basada en su relación semántica. 
    MonoT5, por otro lado, reformula la tarea de re-ranking como un problema de clasificación, asignando una probabilidad a cada documento según su pertinencia. 
    Otra alternativa es ColBERT (Contextualized Late Interaction BERT), que optimiza la comparación de embeddings para equilibrar precisión y eficiencia. 
    Aunque estos modelos mejoran significativamente la calidad del ranking, requieren más recursos computacionales, lo que puede dificultar su implementación en sistemas de respuesta en tiempo real sin optimización adecuada.

    \item \textbf{Re-ranking basado en feedback y aprendizaje reforzado}: Este enfoque se basa en el ajuste dinámico del ranking mediante el aprendizaje a partir de datos históricos o interacciones del usuario. 
    En el aprendizaje supervisado, los modelos son entrenados con conjuntos de datos etiquetados, donde se identifican manualmente los documentos más relevantes para ciertas consultas. 
    Por otro lado, los sistemas interactivos pueden aprovechar el feedback del usuario, analizando clics, tiempo de lectura o valoraciones para ajustar la selección de documentos. 
    Además, el aprendizaje por refuerzo permite que el modelo refine su estrategia de re-ranking con el tiempo, optimizando la recuperación mediante técnicas como bandits contextuales o algoritmos de refuerzo como REINFORCE. 
    Este enfoque es ideal para sistemas en evolución continua, ya que mejora la precisión de la información recuperada conforme se acumulan datos de uso real.

\end{itemize}


\subsubsection{Contextual RAG}




paper https://arxiv.org/pdf/2312.10997


introduccion a RAG 

tipos de rag segun el paper: naive, advanced y modular

tecnicas de mejora de rag

busqueda hibrida

reranking

contextual rag

combinacion con sql





% \subsection{Agentes Inteligentes}

% \section{Tecnologías y métodos actuales}

% \section{conclusiones}