\chapter{Estado del Arte}
\label{chapter:Estado del Arte}


%%% SECTION
\section{Introducción}
El objetivo de este capítulo es realizar un análisis de los diferentes avances, desarrollos y tecnologías disponibles en el ámbito de la solución planteada. 

Este análisis tiene como objetivo identificar enfoques y metodologías en distintas áreas, como la extracción de información a partir de fuentes web, el análisis y procesado de texto y el uso de técnicas de Inteligencia Artificial aplicadas al Procesamiento de Lenguaje Natural.

De esta forma se puede establecer un contexto para el problema a resolver y justificar la elección de las tecnologías y metodologías a utilizar en el desarrollo de la solución propuesta.

\section{Problemática a resolver}

La búsqueda de ayudas y subvenciones es una tarea que la mayoría de las empresas, sobre todo las que tienen menos recursos, realizan en su día a día.
Para ello, existen diferentes plataformas de ayudas a empresas, algunas nacionales y otras de carácter internacional. 
Sin embargo, esta tarea puede resultar complicada y tediosa, ya que implica una búsqueda constante de nuevas posibilidades de financiacióna través de distintas fuentes.
Además, la información sobre estas convocatorias suele estar distribuidas en diferentes fuentes, desde las propias plataformas a documentación oficial del estado.
Esto supone que a la hora de realizar una búsqueda de posibles convocatorias de financiación, se acabe con un conjunto de fuentes con diferentes estructuras y formatos.

En la mayoría de los casos, las convocatorias suelen tener asociados diferentes documentos, en su mayoría en formato PDF, los cuales pueden ser extensos, y además usan un lenguaje técnico, típico de este tipo de documentos, que dificulta su comprensión.
Esto al final supone una complicación por parte de las empresas a la hora de acceder a información clave de las convocatorias de forma mas rápida, como requisitos, plazos, presupuesto o condiciones de participación.
Estos problemas de accesibilidad y estandarización de las convocatorias de ayudas suponen una barrera de acceso importante, que reduce las oportunidades de acceso a financiación para algunas empresas, y suponen una inversión en tiempo y esfuerzo en la tarea de búsqueda y filtrado por parte de éstas.

El desarrollo planteado en este proyecto pretende ser una solución a esta problemática, proporcionando una herramienta que sea capaz de identificar y extraer la documentación de las convocatorias, y aplicar técnicas de Inteligencia Artificial para extraer la información clave y dotarla de una estructura mas estandarizada, así como permitir la consulta de esta información de forma sencilla a partir de un agente conversacional.


\section{Soluciones disponibles}

Dejando a parte de momento las soluciones basadas en Inteligencia Artificial, las cuales se comentarán en secciones posteriores, existen diferentes metodologías para la búsqueda de información sobre estas convocatorias:

\subsection{Plataformas de convocatorias}

Existen diferentes plataformas que recopilan información sobre convocatorias de ayudas y subvenciones, a las cuales las empresas pueden acceder para explorar las diferentes opciones de convocatorias, y valorar si se ajustan a su situación.
Estos portales de convocatorias suelen estar disponibles en plataformas tanto gubernamentales como privadas, que recopilan y organizan la información sobre diferentes ayudas disponibles. 
Además, estas herramientas suelen permitir aplicar filtros en las búsquedas por diferentes características, como el área geográfica, el perfil de la empresa solicitante, o el tipo de ayuda.

\begin{itemize}

    \item \textbf{Portales de ayudas gubernamentales}:\\
    Las diferentes instituciones públicas suelen ofrecer portales informativos donde publican este tipo d econvocatorias, ya sean a nivel local, regional o nacional. Estos portales permiten visualizar estas voncovatorias, pero a un nivel básico en cuanto a experiencia de usuario, y aunque la totalidad de la información siempre está disponible, ya sea en el propio portal o mediante enlaces a diferentes fuentes documentales, el análisis y búsqueda de información clave es tediosa y lenta.    
    
    \begin{itemize}
        \item CDTI: Centro para el Desarrollo Tecnológico y la Innovación\cite{cdti}.
        \item Grupo SPRI\cite{spri}.
        \item SODERCAN: Sociedad para el desarrollo ragional de Cantabria\cite{sodercan}.
        \item Portal de ayudas del Ministerio para la Transofrmación Digital y de la Función Pública\cite{ayudasgob}.
        \item Andalucía Trade: Incentivos para Desarrollo Induatrial y Proyectos de I+D+i \\Empresarial\cite{atrade}.
    \end{itemize}

    \item \textbf{Plataformas privadas de información sobre subvenciones}:\\
    Algunas empresas recopilan información sobre ayudas a empresas, las estructuran en bases de datos y ofrecen el acceso a esta información como servicio, garantizando en éste la calidad de la información y una actualización constante del listado de ayudas disponibles.
    El inconveniente de estas plataformas es que, pese a ofrecer servicios de búsqueda que suelen tener interfaces mas amigables e información mas directa, suelen ser herramientas de pago, y el acceso completo a la información puede supoer un coste económico adicional.
    Algunos ejemplos de estas plataformas son Fandit \cite{fandit} u OpenGrants \cite{opengrants}.

\end{itemize}


\subsection{Web Scraping}

Una solución alternativa a la búsqueda manual en portales de ayudas es el uso de herramientas de Web Scraping.
El Web Scraping \cite{inbook} es una técnica utilizada para extraer información de sitios web de manera automatizada. Consiste en el uso de programas o scripts que navegan por páginas web, recopilan datos estructurados y los almacenan en un formato más accesible, como bases de datos o archivos locales JSON o CSV, por ejemplo. Esta práctica es ampliamente utilizada en diversos sectores para la recopilación y análisis de información a gran escala.

Generalmente el proceso de Web Scraping se desarrolla en varias etapas:

\begin{itemize}
    \item Solicitud HTTP: La herramienta de scraping envía una solitcitud HTTP a una página web para obtener su contenido.
    \item Extracción de datos: Se analiza el código fuente de la página, y según la configuración establecida en el scraper, se extraen los datos requeridos mediante comandos de parseo propios de la herramienta, expresiones regulares, o bots de navegación automatizada.
    \item Almacenamiento de la información: Una vez obtenidos los datos, estos se pueden formatear y almacenar según convenga en el caso de uso. 
\end{itemize}

Existen diferentes metodologías de Web Scraping:

\begin{itemize}
    \item \textbf{Análisis HTML}: En múltiples sitios web, se generan automáticamente grandes volúmenes de páginas a partir de fuentes de datos estructuradas, como bases de datos, mediante scripts o plantillas que organizan la información en formatos homogéneos. 
    En minería de datos, un wrapper es un programa que identifica plantillas en una fuente de datos, extrae su contenido y lo transforma en una estructura relacional. 
    La inducción de wrappers asume que las páginas de entrada siguen un patrón identificable, usualmente a través de formatos de URL comunes. 
    Además, lenguajes de consulta para datos semiestructurados, como XQuery y HTQL, permiten analizar, extraer y modificar información en sitios web HTML \cite{miningweb}.

    \item \textbf{Análisis DOM}: Los programas pueden acceder a contenido dinámico generado por scripts del lado del cliente mediante la integración de un navegador web, como Internet Explorer o Mozilla browser control \cite{domparsing}. 
    Estas aplicaciones analizan las páginas web y las estructuran en un árbol del Document Object Model (DOM), lo que permite extraer secciones específicas del contenido.
    El modelo DOM organiza una página web en una estructura arbórea, permitiendo su interpretación y almacenamiento a partir de una dirección web especificada, como ocurre en los motores de búsqueda. 
    Este enfoque ofrece gran flexibilidad y agilidad, ya que permite rastrear elementos presentes en la página sin depender de que el equipo de desarrollo web los exponga explícitamente en la capa de datos.

    \item \textbf{HTML DOM (Hyper Text Markup Language Document Object Model)}: Es un estándar para la obtención, manipulación y modificación de elementos HTML \cite{Gunawan}. 
    Define objetos y propiedades para cada componente HTML, así como métodos para acceder a ellos, optimizando la eficiencia del DOM.
    JavaScript, como lenguaje principal, permite acceder y manipular todos los elementos de un documento HTML a través del DOM. 
    En este modelo, cada elemento HTML se trata como un objeto, cuya interfaz de programación está compuesta por métodos y propiedades específicas.

    \item \textbf{Expresiones regulares (RegEx)}: Las expresiones regulares son fórmulas que definen patrones específicos para identificar conjuntos de caracteres en diversas cadenas de texto \cite{oreilywebacraping}. 
    Se componen de caracteres ordinarios y metacaracteres, los cuales modifican la interpretación del patrón.
    Aunque su sintaxis puede parecer compleja, las expresiones regulares son una herramienta esencial para el análisis y procesamiento de datos en cadenas de texto, por lo que es fundamental comprenderlas al menos a nivel básico.
    
    \item \textbf{XPath}: XPath es el componente principal del estándar XSLT (Stylesheet Language Transformation) y se utiliza para navegar y seleccionar elementos y atributos dentro de documentos XML \cite{asikri}. 
    Además, puede aplicarse en documentos HTML.
    XPath funciona como un lenguaje de selección de nodos en estructuras XML, siendo la expresión más utilizada la ruta de ubicación (location path). 
    Esta ruta emplea al menos un paso de ubicación para identificar un conjunto de nodos dentro de un documento. 
    La forma más simple es la selección del nodo raíz del documento, representada por el símbolo "/" , que también es el indicador del directorio raíz en sistemas de archivos Unix.
    
    \item \textbf{Reconocimiento de anotaciones semánticas}: Las páginas extraídas pueden incluir metadatos, marcas semánticas y anotaciones que permiten identificar datos específicos \cite{inbook}. 
    Por ejemplo, esta técnica puede considerarse un caso particular del análisis DOM si las anotaciones están integradas en las páginas, como ocurre con Microformat. 
    En otro caso, las anotaciones se almacenan y gestionan de manera independiente de las páginas web, organizándose en una capa semántica, de modo que los scrapers pueden obtener el esquema e instrucciones desde esta capa antes de realizar el raspado de las páginas.


\end{itemize}

En el ámbito del Web Scraping, específicamente con el lenguaje de Programación Python, podemos encontrar las siguientes librerías:

\begin{itemize}
    \item \textbf{BeautyfulSoup}: BeautifulSoup es una biblioteca de Python diseñada para el análisis, extracción y manipulación de datos en documentos HTML y XML. 
    Su funcionamiento se basa en la creación de un árbol de análisis sintáctico (parse tree), que estructura el contenido de la página web de manera jerárquica, permitiendo navegar por los nodos, buscar elementos específicos y modificar el contenido.
    BeautifulSoup admite múltiples analizadores (parsers), como lxml, html.parser y html5lib, cada uno con diferentes niveles de velocidad y compatibilidad. 
    Su sintaxis flexible permite localizar elementos a través de etiquetas, atributos y selectores CSS, facilitando la extracción de datos estructurados de páginas web. Además, cuenta con métodos para limpiar el contenido, eliminar etiquetas HTML y exportar la información en diversos formatos \cite{9274270}.

    \item \textbf{Scrapy}: Scrapy es un framework de Python diseñado para la extracción estructurada de datos mediante web scraping y crawling. 
    Su arquitectura modular permite gestionar solicitudes HTTP, procesar respuestas y almacenar datos de manera eficiente. 
    Scrapy opera a través de un flujo de trabajo basado en spiders, que son clases definidas por el usuario encargadas de especificar la lógica de extracción \cite{domparsing}.
    
    El motor de Scrapy (Scrapy Engine) coordina los componentes principales:
    
    \begin{itemize}
        \item Scheduler, que organiza las solicitudes pendientes.
        \item Downloader, que ejecuta las peticiones HTTP y recibe las respuestas.
        \item Spiders, que analizan y extraen información relevante.
        \item Item Pipeline, que transforma, valida y almacena los datos obtenidos en formatos como JSON, CSV o bases de datos SQL y NoSQL.
    \end{itemize}

    Además, Scrapy admite el uso de middlewares, tanto en el Downloader como en el Spider, para modificar solicitudes y respuestas, gestionar sesiones y evitar bloqueos mediante técnicas como rotación de proxies y user agents. 
    Su diseño asincrónico optimiza el rendimiento, permitiendo la extracción masiva de datos con alta eficiencia.

    \item \textbf{Selenium}: Selenium es un framework de automatización de navegadores de código abierto utilizado para la ejecución de pruebas y la extracción de datos mediante web scraping \cite{9142938}. 
    Su funcionamiento se basa en la interacción con páginas web a través de un WebDriver, que actúa como un controlador para manipular elementos de la interfaz de usuario, simular clics, completar formularios y desplazarse por el contenido dinámico generado mediante JavaScript.
    El ecosistema de Selenium está compuesto por varios módulos:

    \begin{itemize}
        \item Selenium WebDriver, que permite la automatización de navegadores como Chrome, Firefox y Edge mediante controladores específicos.
        \item Selenium Grid, que posibilita la ejecución distribuida de pruebas y scraping en múltiples máquinas.
        \item Selenium IDE, una extensión que facilita la grabación y reproducción de secuencias de prueba en navegadores.
    \end{itemize}

    Para realizar web scraping con Selenium, se inicia una sesión de navegador con el WebDriver, se navega a la URL objetivo, y se localizan los elementos deseados mediante selectores XPath o CSS. A diferencia de frameworks como Scrapy o BeautifulSoup, Selenium es ideal para interactuar con sitios que requieren ejecución de JavaScript o carga dinámica de contenido, aunque su rendimiento puede ser inferior debido a la sobrecarga computacional del manejo de un navegador real.

\end{itemize}

\subsection{Procesamiento de Lenguaje Natural}

El Procesamiento de Lenguaje Natural (NLP) es una disciplina de la inteligencia artificial y la lingüística computacional que permite a las máquinas interpretar, comprender, generar y manipular el lenguaje humano de manera estructurada \cite{nlpstate}. 
Su aplicación abarca desde la traducción automática y el análisis de sentimientos hasta la generación de texto y los asistentes virtuales.

El Procesamiento de Lenguaje Natural emplea diversas técnicas computacionales para la interpretación y manipulación del lenguaje humano. 
Estas técnicas pueden dividirse en métodos estadísticos, basados en reglas y de aprendizaje profundo, siendo ampliamente utilizadas en tareas como la clasificación de textos, el análisis de sentimientos y la traducción automática.

\begin{itemize}
    \item \textbf{Preprocesamiento de Texto}
        \begin{itemize}
            \item \textbf{Tokenización}: La tokenización es un proceso esencial en el Procesamiento de Lenguaje Natural que divide un texto en unidades denominadas tokens. 
            Existen varios tipos \cite{schmidt2024tokenizationcompression}, siendo los principales: tokenización por palabras, que separa el texto en palabras individuales y es efectiva en idiomas con delimitadores claros como el inglés; 
            tokenización por subpalabras, utilizada en modelos como BERT y GPT, que emplea técnicas como Byte Pair Encoding (BPE) para manejar vocabularios extensos y palabras fuera de vocabulario (OOV); 
            tokenización por oraciones, que segmenta el texto en unidades sintácticas más grandes basándose en puntuación y reglas lingüísticas; y tokenización por caracteres, útil en modelos de aprendizaje profundo y generación de texto. 
            La selección del método adecuado depende del idioma y la tarea específica, siendo crucial en aplicaciones como traducción automática, análisis de sentimientos y recuperación de información.
            
            \item \textbf{Lematización y Stemming}: Son técnicas empleadas para normalizar palabras reducciéndolas a su raíz morfológica \cite{steminglemmatization}. 
            El stemming elimina afijos mediante reglas predefinidas, sin considerar el contexto, lo que lo hace rápido pero propenso a errores (ejemplo: "running" → "run", pero "better" → "bet"). 
            En contraste, la lematización utiliza análisis morfológico y diccionarios lingüísticos para obtener la forma base correcta (ejemplo: "better" → "good"), ofreciendo mayor precisión. 
            Mientras que el stemming es más eficiente en grandes volúmenes de datos, la lematización es preferida en tareas como análisis de sentimientos, recuperación de información y traducción automática, donde la precisión semántica es crucial.
            
            \item \textbf{Eliminación de stopwords}: Las stopwords son palabras de alta frecuencia en un idioma que generalmente no aportan significado relevante en tareas de Procesamiento de Lenguaje Natural, como artículos, preposiciones y pronombres (ejemplo: "el", "de", "y", "pero"). 
            Se eliminan para reducir la dimensionalidad del texto y mejorar la eficiencia de los modelos de análisis de texto \cite{Sarica_2021}. 
            Bibliotecas como NLTK, SpaCy y Scikit-learn incluyen listas de stopwords predefinidas, aunque pueden personalizarse según la aplicación. 
            Si bien su eliminación es útil en tareas como búsqueda de información y clasificación de textos, en ciertos casos, como en análisis de sentimientos o generación de texto, pueden ser necesarias para preservar el contexto semántico.
        \end{itemize}

        \item \textbf{Análisis Morfosintáctico y Semántico}
        \begin{itemize}
            \item \textbf{Etiquetado gramatical (POS Tagging)}: Es una técnica del Procesamiento de Lenguaje Natural que asigna a cada palabra de un texto su categoría gramatical correspondiente (sustantivo, verbo, adjetivo, etc.) en función de su contexto \cite{postagging}. 
            Se basa en reglas lingüísticas, modelos estadísticos o redes neuronales para mejorar la precisión del análisis. 
            Herramientas como NLTK, SpaCy y Stanford NLP utilizan algoritmos como Hidden Markov Models (HMM) o Redes Neuronales Recurrentes (RNNs) para realizar esta tarea. 
            El POS Tagging es clave en aplicaciones como análisis de sentimientos, desambiguación semántica y traducción automática, ya que ayuda a comprender la estructura y significado del lenguaje.
            
            \item \textbf{Parsing sintáctico}: El parsing sintáctico o análisis sintáctico consiste en la construcción de la estructura jerárquica de una oración para comprender su sintaxis, identificando la relación jerárquica entre las palabras mediante árboles sintácticos o dependencias gramaticales \cite{zhang2020surveysyntacticsemanticparsingbased}. 
            Existen dos enfoques principales: el parsing basado en constituyentes, que descompone la oración en frases (sintagmas nominales, verbales, etc.), y el parsing basado en dependencias, que representa las relaciones entre palabras mediante un grafo dirigido. 
            Herramientas como NLTK, SpaCy y Stanford Parser emplean algoritmos como CYK, Earley o modelos de redes neuronales para esta tarea. 
            El parsing sintáctico es esencial en aplicaciones como traducción automática, generación de texto y comprensión del lenguaje natural, donde la estructura de la oración influye en su interpretación.
            
            \item \textbf{Reconocimiento de Entidades Nombradas (NER)}: Es una técnica que tiene como objetivo identificar y clasificar entidades dentro de un texto, como nombres de personas, lugares, organizaciones, fechas, entre otros \cite{roy2021recenttrendsnamedentity}. 
            Utiliza enfoques basados en reglas lingüísticas, modelos estadísticos o aprendizaje automático para detectar estas entidades en el contexto del texto. 
            Herramientas como SpaCy, NLTK y Stanford NER emplean modelos entrenados en grandes corpus de datos para reconocer entidades y asignarles una etiqueta adecuada. 
            El NER es crucial en aplicaciones como extracción de información, análisis de noticias, y búsqueda semántica, ya que facilita la identificación y categorización de información relevante dentro de grandes volúmenes de datos.
        \end{itemize}

        \item \textbf{Representación de Texto}
        \begin{itemize}
            \item \textbf{Bag of Words (BoW)}: Es una técnica de representación de texto que convierte éste en una matriz de características, donde cada documento se representa como un conjunto de palabras sin tener en cuenta el orden o la gramática \cite{bow}. 
            En el modelo BoW, cada palabra única en el corpus se convierte en una característica (o columna) y cada documento se representa como un vector en el que el valor de cada entrada corresponde a la frecuencia de la palabra en ese documento. 
            Aunque es simple y eficiente, BoW presenta varios inconvenientes, como la pérdida de contexto y el orden de las palabras, la alta dimensionalidad, la falta de captura de relaciones semánticas y la presencia de ruido en los datos, lo que puede afectar la precisión y la interpretación del modelo. 
            Esta técnica, sin embargo, sigue siendo útil en tareas como clasificación de texto, análisis de sentimientos y recuperación de información, pero sus limitaciones han llevado al desarrollo de enfoques más avanzados.
            
            \item \textbf{TF-IDF (Term Frequency - Inverse Document Frequency)}: Método que pondera términos relevantes dentro de un corpus \cite{tfidf}. 
            Es una técnica de representación de texto que asigna un peso a cada término de un documento, basándose en dos componentes: la frecuencia de término (TF), que mide cuántas veces aparece un término en un documento, y la frecuencia inversa de documento (IDF), que mide la importancia de un término dentro de un conjunto de documentos. 
            El cálculo de TF es sencillo y se basa en la cantidad de veces que un término aparece en un documento en comparación con el total de términos del documento, mientras que IDF ajusta el peso del término según su frecuencia en todos los documentos, penalizando las palabras comunes que aparecen en muchos documentos. 
            El resultado es un valor que refleja la relevancia de un término en un documento en particular dentro de un corpus. 
            TF-IDF es ampliamente utilizado en tareas como clasificación de texto, búsqueda de información y análisis de contenido, ya que ayuda a identificar términos significativos que son relevantes en el contexto de un conjunto de documentos. 
            Sin embargo, también presenta algunas limitaciones, como la incapacidad para capturar relaciones semánticas entre palabras y su dependencia de la estructura del corpus.
            
            \item \textbf{Word Embeddings}: Técnicas avanzadas de representación de palabras que convierte las palabras en vectores numéricos de alta dimensión \cite{almeida2023wordembeddingssurvey}. 
            A diferencia de las representaciones tradicionales como Bag of Words o TF-IDF, que tratan las palabras de manera independiente, los word embeddings capturan las relaciones semánticas y contextuales entre palabras, representando palabras similares en espacios vectoriales cercanos. 
            Modelos como Word2Vec, GloVe, FastText o los posteriores modelos de Embeddings asociados a Grandes Modelos del Lenguaje aprenden estas representaciones mediante redes neuronales entrenadas sobre grandes corpus de texto, aprovechando el contexto de las palabras en las oraciones para generar sus vectores. 
            Los word embeddings permiten capturar propiedades lingüísticas, como sinónimos, analogías y jerarquías semánticas, mejorando el rendimiento en tareas como traducción automática, análisis de sentimientos, clasificación de texto y respuestas automáticas. 
            Aunque potentes, los embeddings también presentan desafíos, como la dificultad para representar términos poco frecuentes o palabras con múltiples significados (polisemia).
        
        \end{itemize}

        \item \textbf{Modelos de Aprendizaje Automático y Profundo}
        \begin{itemize}
            \item \textbf{Modelos basados en aprendizaje automático clásico}: Utilización de algoritmos tanto supervisados como no supervisados, como regresiones logísticas, árboles de decisión, SVM, k-means o Random Forest, entre otros, para tareas de clasificación o clusterización de texto \cite{nlpml}.
            Estos modelos estan diseñados para trabajar con datos numéricos, por lo que es necesario aplicar técnicas de representación numérica de texto como las ya anteriormente comentadas.
            
            \item \textbf{Redes Neuronales Recurrentes (RNN) y LSTM}: Modelos diseñados para procesar secuencias de texto y capturar dependencias contextuales.
            Son arquitecturas de Deep Learning diseñadas para procesar datos secuenciales, lo que las hace especialmente útiles en Procesamiento de Lenguaje Natural. 
            A diferencia de los modelos tradicionales de Machine Learning, las RNN pueden capturar dependencias temporales en el texto, ya que su estructura permite que la información de estados previos influya en la interpretación de los siguientes \cite{schmidt2019recurrentneuralnetworksrnns}. 
            Sin embargo, las RNN convencionales presentan problemas con secuencias largas debido a la desaparición o explosión del gradiente. 
            Para solucionar esto, surgieron las LSTM, que incorporan puertas de memoria que regulan el flujo de información, permitiendo recordar dependencias a largo plazo de manera más eficiente \cite{staudemeyer2019understandinglstmtutorial}. 
            Estas redes han sido ampliamente utilizadas en tareas como traducción automática, generación de texto, análisis de sentimientos y reconocimiento de voz. 
            Aunque las LSTM han mejorado el manejo de secuencias largas, han sido en gran parte reemplazadas por arquitecturas más avanzadas como Transformers, que manejan contexto de manera más eficiente mediante mecanismos de atención.
            
            \item \textbf{Arquitecturas de Deep Learning basadas en Transformers}: Estas nuevas arquitecturas revolucionaron el Procesamiento de Lenguaje Natural al superar las limitaciones de las arquitecturas de Redes Neuronales anteriores, gracias a su capacidad para procesar secuencias en paralelo y capturar relaciones a largo plazo mediante el mecanismo de atención. 
            Introducidos en el paper "Attention Is All You Need" \cite{vaswani2023attentionneed}, los Transformers utilizan mecanismos de self-attention para asignar pesos a cada palabra en función de su relevancia dentro del contexto, lo que permite una comprensión más profunda del significado del texto. 
            Modelos como BERT (Bidirectional Encoder Representations from Transformers \cite{devlin2019bertpretrainingdeepbidirectional}), GPT (Generative Pre-trained Transformer \cite{yenduri2023generativepretrainedtransformercomprehensive}) y T5 (Text-to-Text Transfer Transformer \cite{raffel2023exploringlimitstransferlearning}) lograron en su momento avances significativos en tareas como traducción automática, generación de texto, análisis de sentimientos y respuesta a preguntas. 
            Estas arquitecturas destacan por su capacidad de preentrenamiento en grandes corpus de datos y posterior ajuste fino en tareas específicas, lo que ha permitido obtener resultados de vanguardia en múltiples aplicaciones de PLN. 
            Tomando como base estas arquitecturas, han aparecido en los últimos años los Grandes Modelos del Lenguaje (LLMs), dando origen al desarrollo de la llamada Inteligencia Artificial Generativa.
        \end{itemize}

\end{itemize}

Estas técnicas han permitido avances en aplicaciones como asistentes virtuales, generación de texto automatizada y motores de búsqueda, optimizando la interacción entre humanos y sistemas computacionales.






\section{Inteligencia Artificial Generativa}

% \subsection{Grandes Modelos del Lenguaje (LLMs)}

% \subsection{Prompt Engineering}

% \subsection{Retrieval Augmented Generation (RAG)}

% \subsection{Agentes Inteligentes}

% \section{Tecnologías y métodos actuales}

% \section{conclusiones}